<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>altriavin の blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="description" content="I am learning ML and DL by watching video on YouTube. There is my study notes. In the first lesson, he mainly introduce the concept of ML. And then, an example of linear models is given to help us open the door to ML.">
  
  
  
    <link rel="shortcut icon" href="../../../../altriavin.ico">
  
  
    
<link rel="stylesheet" href="../../../../fancybox/jquery.fancybox-1.3.4.css">

  
  
<link rel="stylesheet" href="../../../../css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="../../../../index.html" id="logo">altriavin の blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="../../../../index.html" id="subtitle">student, friend, learning and life</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-LHY ML 1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="" class="article-date">
  <time class="dt-published" datetime="2021-11-21T06:58:36.558Z" itemprop="datePublished">2021-11-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>I am learning ML and DL by watching video on YouTube. There is my study notes. In the first lesson, he mainly introduce the concept of ML. And then, an example of linear models is given to help us open the door to ML.</p>
<span id="more"></span>
<h3 id="the-type-of-ml">The type of ML</h3>
<p><strong>Regression</strong>: The function outputs a scalar.</p>
<p><strong>Classification</strong>: Given options(classes), the function outputs the correct one.</p>
<p><strong>structured learning</strong>: create something with structure</p>
<h3 id="the-example-of-linear-model">The example of Linear model</h3>
<p>a example for the Machine learning -- predict the youtube channels</p>
<h4 id="function-with-unknown">Function with unknown</h4>
<p><span class="math display">\[
y = b + wx_1
\]</span></p>
<p>which <span class="math inline">\(y\)</span> represents the total views we will get today, <span class="math inline">\(x_1\)</span> is the number of views that we got on this channels yesterday, <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> is unknown parameters.</p>
<h4 id="define-loss-from-training-data">Define Loss from Training Data</h4>
<p>Loss is a function on parameters <span class="math inline">\(L(b,w)\)</span></p>
<p>for example, we can define the loss function as follow: <span class="math display">\[
\widehat{y_i} = b_1 + w_1x_i
\]</span> <span class="math display">\[
y_i = b_1 + w_1x_i
\]</span></p>
<p><span class="math display">\[
e_i = |{\widehat{y_i} - y_i}|
\]</span></p>
<p><span class="math display">\[
L = \frac{1}{N} \sum_{i=1}^{N}e_i
\]</span></p>
<p>which <span class="math inline">\(\widehat{y_i}\)</span> represent the <span class="math inline">\(ith\)</span> predictive value. <span class="math inline">\(y_i\)</span> is the <span class="math inline">\(ith\)</span> true value. <span class="math inline">\(e_i\)</span> is the the mean absolute error(MAE) of <span class="math inline">\(\widehat{y_i}\)</span> and <span class="math inline">\(y_i\)</span>, in other case, we can define <span class="math inline">\(e_i = (\widehat{y_i} - y_i)^2\)</span>, which is the mean square error(MSE). In that case, we use MAE. <span class="math inline">\(Loss\)</span> is the average of the <span class="math inline">\(e_i\)</span>.</p>
<h4 id="optimization">Optimization</h4>
<p><span class="math display">\[
w^*, b^* = arg \min_{w,b}L
\]</span></p>
<p>which the <span class="math inline">\(w^*\)</span> and <span class="math inline">\(b^*\)</span> is the <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> which can make the loss smallest.</p>
<p>The way to find the best <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> is called <strong>Gradient Descent</strong>.</p>
<p>How to use the Gradient Descent?</p>
<p>Firstly, we suppose that there is only one paramenter <span class="math inline">\(w\)</span>. Then, we can get the error surface about <span class="math inline">\(L\)</span> and <span class="math inline">\(w\)</span> as follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/the%20loss%20between%20L%20and%20w.jpeg" /></p>
<center>
fig 1: the loss between <span class="math inline">\(L\)</span> and <span class="math inline">\(w\)</span>
</center>
<p>After that, we need to randomly select an initial point called <span class="math inline">\(w_0\)</span> and compute the derivative of loss with respect to <span class="math inline">\(w\)</span> as follow: <span class="math display">\[
\frac{\partial{L}}{\partial{w}}|_{w=w^0}
\]</span> If the result is negative, we need to increase <span class="math inline">\(w\)</span>, others, we need to decrease <span class="math inline">\(w\)</span>. And the step size as follow: <span class="math display">\[
\eta \frac{\partial{L}}{\partial{w}}|_{w=w^0}
\]</span> which <span class="math inline">\(\eta\)</span> represent the learning rate, it depends on you.</p>
<p>The next step is to move to the new position which is called <span class="math inline">\(w_1\)</span>, the formula as follow: <span class="math display">\[
w_1 =
\begin{cases}
w_0 - \eta \frac{\partial{L}}{\partial{w}}|_{w=w^0} &amp; {\frac{\partial{L}}{\partial{w}} &gt;= 0}\\
w_0 + \eta \frac{\partial{L}}{\partial{w}} &amp; {\frac{\partial{L}}{\partial{w}} &lt; 0}
\end{cases}
\]</span> After that, we need to update <span class="math inline">\(w\)</span> iteratively.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/update%20the%20w%20iteratively.jpeg" /></p>
<center>
fig 2: update the w iteratively
</center>
<p><strong>Gradient Descent</strong> is not a prefect method that has the problem of local minima.</p>
<p>Back to our problem, we actually have two parameters, <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span>. We can get the best <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> follow the steps below.</p>
<ol type="1">
<li>(Randomly) Pick initial values <span class="math inline">\(w^0\)</span>, <span class="math inline">\(b^0\)</span></li>
<li>compute</li>
</ol>
<p><span class="math display">\[
\frac{\partial L}{\partial w}|_{w=w^0,b=b_0}
\]</span></p>
<p><span class="math display">\[
\frac{\partial L}{\partial b}|_{w=w^0,b=b_0}
\]</span></p>
<p>and then, we can compute the <span class="math inline">\(w_1\)</span> and <span class="math inline">\(b_1\)</span> as the follow formula. <span class="math display">\[
w_1 = w_0 - \eta \frac{\partial L}{\partial w}|_{w=w^0,b=b^0}
\]</span></p>
<p><span class="math display">\[
b_1 = b_0 - \eta \frac{\partial L}{\partial b}|_{w=w^0,b=b^0}
\]</span></p>
<ol start="3" type="1">
<li>update <span class="math inline">\(w\)</span> and <span class="math inline">\(b\)</span> iteratively</li>
</ol>
<p>After the above step, we can get the error surface as follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/the%20error%20surface%20of%20the%20w%20and%20b.jpeg" /></p>
<center>
fig 3: the error surface of the w and b
</center>

      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../../22/LHY%20ML%202/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
    <a href="../../../02/08/to%20fo%20list/LHY%20ML%204/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 altriavin<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a><br>
      <a target="_blank" rel="noopener" href="https://beian.miit.gov.cn"></a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
</nav>
    


<script src="../../../../js/jquery-1.4.3.min.js"></script>


  
<script src="../../../../fancybox/jquery.fancybox-1.3.4.js"></script>




<script src="../../../../js/script.js"></script>






<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

  </div>
</body>
</html>