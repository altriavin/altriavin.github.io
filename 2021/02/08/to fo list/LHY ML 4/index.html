<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>altriavin の blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
  <meta name="description" content="Classification: Probabilistic Generative ModelTake a example, input a Pokémon, and output the Pokémon’s type, as Figure 1 follow.

Figure 1: the example of Classification">
  
  
  
    <link rel="shortcut icon" href="../../../../../altriavin.ico">
  
  
    
<link rel="stylesheet" href="../../../../../fancybox/jquery.fancybox-1.3.4.css">

  
  
<link rel="stylesheet" href="../../../../../css/style.css">

<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="../../../../../index.html" id="logo">altriavin の blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="../../../../../index.html" id="subtitle">student, friend, learning and life</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
      </nav>
      <nav id="sub-nav">
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-to fo list/LHY ML 4" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="" class="article-date">
  <time class="dt-published" datetime="2021-02-08T01:19:29.000Z" itemprop="datePublished">2021-02-08</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="Classification-Probabilistic-Generative-Model"><a href="#Classification-Probabilistic-Generative-Model" class="headerlink" title="Classification: Probabilistic Generative Model"></a>Classification: Probabilistic Generative Model</h2><p>Take a example, input a Pokémon, and output the Pokémon’s type, as Figure 1 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112161621024.png"></p>
<center>Figure 1: the example of Classification</center>

<span id="more"></span>

<p>So, how to do this?</p>
<p>First, we need to numericalize Pokémon and then, we can use Pokémon as input to the function. To every Pokémon, we can use a vector which has 7 value(<strong>Total, HP, Attack, Defense, SP Atk, SP Def and Speed</strong>) to describe it.</p>
<h3 id="Training-data-for-Classification"><a href="#Training-data-for-Classification" class="headerlink" title="Training data for Classification"></a>Training data for Classification</h3><p>The training data is just a pair, like $(x_1, \hat{y_1})$, the $x_1$ is a Pokémon and the output is the Pokémon’s type.</p>
<h3 id="Idea-1-Classification-as-Regression"><a href="#Idea-1-Classification-as-Regression" class="headerlink" title="Idea 1: Classification as Regression?"></a>Idea 1: Classification as Regression?</h3><p>We can take binary classification as example, assuming the Pokémon has just two type. </p>
<ul>
<li>In training step, we define the class 1 means the target is 1 and the class 2 means the target is -1.</li>
<li>In testing step, if the predictive value is closer to 1, the class is 1; if it is closer to -1, the class is 2.</li>
</ul>
<p>If you are luck, the data is very good, you will get the straight line, that is  $b + w_1 x_1 + w_2x_2 = 0$ , as shown on the left side of Figure 2. Unfortunately, the data is always bad. Like the right of Figure 2, there are some data that predictive value is much greater than 1, and there data will be error. After training, we will get the purple line, not the green line.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112161653516.png"></p>
<center>Figure 2: the example of Idea 1</center>

<p>Obviously, this idea is not feasible.</p>
<h3 id="Ideal-Alternatives"><a href="#Ideal-Alternatives" class="headerlink" title="Ideal Alternatives"></a>Ideal Alternatives</h3><p>The ideal alternatives as Figure 3 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112170918778.png"></p>
<center>Figure 3: Ideal Alternatives</center>

<h3 id="Probability-Theory-based-Method"><a href="#Probability-Theory-based-Method" class="headerlink" title="Probability Theory-based Method"></a>Probability Theory-based Method</h3><h4 id="Two-Boxes"><a href="#Two-Boxes" class="headerlink" title="Two Boxes"></a>Two Boxes</h4><p>Supposing there are two boxes, Box1 has four blue ball and one green ball, Box2 has two blue ball and two green ball, as Figure 4 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112170931636.png"></p>
<center>Figure 4: Two Boxes</center>

<p>Now, we know that the probability of getting a ball from Box1 is $P(B_1) = \frac{2}{3}$, and  Box2 is $P(B_2) = \frac{1}{3}$, $P(Blue|B_1) = \frac{4}{5}$, $P(Green|B_1) = \frac{1}{5}$, $P(Blue|B_2) = \frac{2}{5}$, $P(Green|B_2) = \frac{3}{5}$.</p>
<p>And then, we can get the probability of getting a blue ball, and it comes from Box1 is<br>$$<br>P(B_1|Blue) = \frac{P(Blue|B_1)P(B_1)}{P(Blue|B_1)P(B_1) + P(Blue|B_2)P(B_2)}<br>$$</p>
<h4 id="Two-Class"><a href="#Two-Class" class="headerlink" title="Two Class"></a>Two Class</h4><p>Now, we can replace the box to class as Figure 5 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112170944492.png"></p>
<center>Figure 5: Two Class</center>

<p>Now, the problem is given an $x$, which class does it belong to?</p>
<p>Like Two Boxes, we can get the probablity of $x$ belongs to Class 1 is<br>$$<br>P(C_1|x) = \frac{P(x|C_1)P(C_1)}{P(x|C_1)P(C_1) + P(x|C_2)P(C_2)}<br>$$<br> And we need to estimating the probabilities from training data to get $P(C_1)$ and others.</p>
<p>And all the idea is called <strong>Generative Model</strong><br>$$<br>P(x) = P(x|C_1)P(C_1) + P(x|C_2)P(C_2)<br>$$</p>
<h4 id="Gaussian-Distribution"><a href="#Gaussian-Distribution" class="headerlink" title="Gaussian Distribution"></a>Gaussian Distribution</h4><p>$$<br>f_{\mu \Sigma}(x) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu)^{T} \Sigma ^{-1} (x - \mu)}<br>$$</p>
<p>The formula of Gaussian distribution as above. The input is a vector $x$, and the output is the probability of sampling $x$.</p>
<p>The shape of the function determines by <strong>mean $\mu$</strong> and <strong>convariance matrix $\Sigma$</strong> as Figure 6 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112171111823.png"></p>
<center>Figure 6: the shape of the function on difficult $\mu$ and $\sum$ </center>

<h4 id="Maximum-Likelihood"><a href="#Maximum-Likelihood" class="headerlink" title="Maximum Likelihood"></a>Maximum Likelihood</h4><p>We can likelihood of a Gaussian with <strong>mean $\mu$</strong>, and convariance <strong>matrix $\sum$</strong>, and then, we will get the probability of the Gaussian samples.<br>$$<br>L(\mu, \Sigma) = \prod f_{\mu, \Sigma}(x^i)<br>$$</p>
<h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><p>Now, let’s do a actual example. There are 79 “water” Pokémon, and 61 “Normal” Pokémon. The task is to predient a probability of  a unknown Pokémon’s type is “water”.</p>
<p> And use $x^{i}$ to replace the 79 “water” Pokémon. We assume $x^{1}, x^{2}, … , x^{79}$ generate from the <strong>Gaussian</strong> $(\mu ^{<em>}, \Sigma ^{</em>})$ with the <strong>maximum likelihood</strong>, which<br>$$<br>\mu ^{<em>}, \Sigma^{</em>} = arg \max_{\mu, \Sigma} L(\mu, \Sigma)<br>$$<br>And then, we can get the $\mu ^{<em>}$ is the average of $x^{i}$.<br>$$<br>\mu ^{</em>} = \frac{1}{79}\sum_{n=1}^{79}x^{n}<br>$$<br>And the $\Sigma ^{<em>}$ is<br>$$<br>\Sigma ^{</em>} = \frac{1}{79} \sum_{n=1}^{79} (x_{n} - \mu ^{<em>})(x_{n} - \mu ^{</em>}) ^ {T}<br>$$<br>Take the actual value, we can compute the “Water” and “Normal” type’s Pokémon. Take the <strong>Defense</strong> and <strong>SP Defense</strong>, we can get the scatter plot as Figure 7 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112171335108.png"></p>
<center>Figure 7: the scatter plot of "Water" and "Normal" type's Pokémon</center>

<p>And use above formula, we can compute the $\mu ^{1}, \Sigma ^{1}$ and $\mu ^{2}, \Sigma ^{2}$ as follow.<br>$$<br>\mu ^{1} =<br>\left[<br>\begin{matrix}<br>75.0 \<br>71.3<br>\end{matrix}<br>\right]<br>\ \ \ \ \<br>\Sigma ^{1} =<br>\left[<br>\begin{matrix}<br>874 &amp; 327 \<br>327 &amp; 929<br>\end{matrix}<br>\right]<br>$$</p>
<p>$$<br>\mu ^{2} =<br>\left[<br>\begin{matrix}<br>55.6 \<br>59.8<br>\end{matrix}<br>\right]<br>\ \ \ \ \<br>\Sigma ^{2} =<br>\left[<br>\begin{matrix}<br>847 &amp; 422 \<br>422 &amp; 685<br>\end{matrix}<br>\right]<br>$$</p>
<p><strong>Now, let’s do classification!</strong></p>
<p>We can get the $P(C_1)$ and $P(C_2)$ as follow.<br>$$<br>P(C_1) = \frac{79}{79 + 61} = 0.56<br>$$</p>
<p>$$<br>P(C_2) = \frac{61}{79 + 61} = 0.44<br>$$</p>
<p>And use <strong>Gaussian Distribution</strong>, we can get the $P(x|C_{1})$ and $P(x|C_2)$ as follow.<br>$$<br>P(x|C_{1}) = f_{\mu ^{1} \Sigma^{1}} (x)<br>$$</p>
<p>$$<br>P(x|C_{2}) = f_{\mu ^{2} \Sigma ^{2}} (x)<br>$$</p>
<p>Then, we can compute the $P(C_{1} | x)$  as follow.<br>$$<br>P(C_1 | x) = \frac{P(x|C_{1})P(C_1)}{P(x|C_{1})P(C_1) + P(x|C_{2})P(C_2)}<br>$$<br>If the $P(C_1 | x) &gt; 0.5$, we can think that $x$ Belong to Class 1(Water). The result as Figure 8 follow.The redder the curve, the more likely it belongs to class 1(water). But the accuracy is bad, just 47%.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112171417541.png"></p>
<center>Figure 8: result</center>

<p>If we use all the value of Pokémon, the accuracy is always bad, just 54%.</p>
<h4 id="Modifying-model-use-the-same-Sigma"><a href="#Modifying-model-use-the-same-Sigma" class="headerlink" title="Modifying model: use the same $\Sigma$"></a>Modifying model: use the same $\Sigma$</h4><p>The above model is bad, but the theory is good. So, we can optimize the model to get a better accuracy, for example, all the $x$ use the same $\Sigma$, as Figure 9 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112171407612.png"></p>
<center>Figure 9: the example of use the same $\Sigma$</center>

<p>And then, we need to Find $\mu ^{1}, \mu ^{2}, \Sigma$ Maximizing the likelihood $L(\mu ^{1}, \mu ^{2}, \Sigma)$.<br>$$<br>L(\mu ^{1}, \mu ^{2}, \Sigma) = \prod_{i=1}^{79} f_{\mu^{1},\Sigma}(x^{i}) \prod_{i=80}^{140}f_{\mu^{2} \Sigma}(x^i)<br>$$<br>And the $\mu ^{1}$ and $\mu ^{2}$ is the same. the $\Sigma$ is<br>$$<br>\Sigma = \frac{79}{140}\Sigma^{1} + \frac{61}{140} \Sigma^{2}<br>$$<br>And then, the result as Figure 10 follow.</p>
<p><img src="https://cdn.jsdelivr.net/gh/altriavin/PictureBed/img/202112171419322.png"></p>
<center>Figure 10: the result of the new model</center>

<p>We can see that the boundary is linear!</p>
<p>If we use all the feature, we will get 73% accuracy instead of 54%!</p>
<h4 id="Posterior-Probability"><a href="#Posterior-Probability" class="headerlink" title="Posterior Probability"></a>Posterior Probability</h4><p>$$<br>P(C_{1}|x) = \frac{P(x|C_{1}) P(C_{1})}{P(x|C_{1}) P(C_{1}) + P(x|C_{2}) P(C_{2})} = \frac{1}{1 + \frac{P(x|C_{2})P(C_{2})}{P(x|C_{1})P(C_{1})}}<br>$$</p>
<p>And define $z$, its value is<br>$$<br>z = ln \frac {P(x|C_{1}) P(C_{1})}{P(x|C_{2}) P(C_{2})}<br>$$<br>So, the $P(C_{1} | x)$ is here.<br>$$<br>P(C_{1} | x) = \frac{1}{1 + e^{-z}} = \sigma (z)<br>$$<br>We can further simplify $z$.<br>$$<br>z = ln \frac{P(x|C_{1})}{P(x|C_{2})} + ln \frac{P(C_{1})}{P(C_{2})}<br>$$</p>
<p>$$<br>ln \frac{P(C_{1})}{P({C_{2}})} = ln \frac{\frac{N_1}{N_1 + N_2}}{\frac{N_{2}}{N_{1} + N_{2}}} = ln \frac{N_{1}}{N_{2}}<br>$$</p>
<p>$$<br>P(x|C_{1}) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma ^{1}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu ^{1})^{T} (\Sigma ^{1}) ^{-1} (x - \mu ^{1})}<br>$$</p>
<p>$$<br>P(x|C_{1}) = \frac{1}{(2 \pi)^{\frac{D}{2}}} \frac{1}{|\Sigma ^{2}|^{\frac{1}{2}}} e^{-\frac{1}{2}(x-\mu ^{2})^{T} (\Sigma ^{2}) ^{-1} (x - \mu ^{2})}<br>$$</p>
<p>Remember? we use the same $\Sigma$, and $\Sigma ^{1} = \Sigma ^{2} = \Sigma$. And we can simplify the $z$ as follow.<br>$$<br>z = (\mu ^{1} - \mu ^{2})^{T} \Sigma ^{-1} x - \frac{1}{2}(\mu ^{1})^{T}\Sigma ^{-1} \mu ^{1} + \frac{1}{2} (\mu ^{2}) ^{T}(\Sigma^{2})^{-1} \mu ^{2} + ln \frac{N_{1}}{N_{2}}<br>$$<br>It’s very long and complex formula, but to be easy. As we all know, $\mu ^{1}, \mu^ {2}, \Sigma$ is constant.</p>
<p>So, we can define $W^{T}$ and $b$ as follow.<br>$$<br>W^{T} = (\mu ^{1} - \mu ^{2})^{T} \Sigma^{-1}<br>$$</p>
<p>$$<br>b = - \frac{1}{2}(\mu ^{1})^{T}\Sigma ^{-1} \mu ^{1} + \frac{1}{2} (\mu ^{2}) ^{T}(\Sigma^{2})^{-1} \mu ^{2} + ln \frac{N_{1}}{N_{2}}<br>$$</p>
<p>Surprise? We can get $P(C_{1}|x)$ as follow.<br>$$<br>P(C_{1}|x) = \sigma (w * x + b)<br>$$</p>
<h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3>
      
    </div>
    <footer class="article-footer">
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="../../../../11/21/LHY%20ML%201/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          (no title)
        
      </div>
    </a>
  
  
</nav>

  
</article>


</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2022 altriavin<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a><br>
      <a target="_blank" rel="noopener" href="https://beian.miit.gov.cn">皖ICP备2021013786号</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
</nav>
    


<script src="../../../../../js/jquery-1.4.3.min.js"></script>


  
<script src="../../../../../fancybox/jquery.fancybox-1.3.4.js"></script>




<script src="../../../../../js/script.js"></script>






<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
</script>
<script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

  </div>
</body>
</html>